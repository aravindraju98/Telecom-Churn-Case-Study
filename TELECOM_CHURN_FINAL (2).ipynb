{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b82422",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the required libraries \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.feature_selection import RFE\n",
    "# Set pandas display options to show more rows and columns to show all the records as needed\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f35386",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the input data \n",
    "df=pd.read_csv('telecom_churn_data.csv')\n",
    "df.info()\n",
    "\n",
    "## data has around 230 columna and 100K rows. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910a3546",
   "metadata": {},
   "source": [
    "### Understanding the input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b08172",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e8b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for the NULL values in the columns\n",
    "df.isna().sum().any()\n",
    "\n",
    "## There are NULL values present in the columns, lets check which all columns have the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba16d144",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().sum()/len(inp_fl)*100)\n",
    "\n",
    "## Most of the columns have less that 10% of the NULL values present in it, so it should be good enough to use those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de67e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check if we have unique customers \n",
    "df.mobile_number.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ce1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700dcc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Columns of importance to tag as churn \n",
    "##  total_ic_mou_9, total_og_mou_9, vol_2g_mb_9, vol_3g_mb_9\n",
    "\n",
    "churn_check=df[['mobile_number','total_ic_mou_9', 'total_og_mou_9', 'vol_2g_mb_9', 'vol_3g_mb_9']]\n",
    "churn_check.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0f3c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b8ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_check['churn']=np.where((churn_check['total_ic_mou_9']==0) &\n",
    "                              (churn_check['total_og_mou_9']==0) &\n",
    "                              (churn_check['vol_2g_mb_9']==0) &\n",
    "                              (churn_check['vol_3g_mb_9']==0), 1,0)\n",
    "churn_check.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e4363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(churn_check['churn'].value_counts()/len(churn_check)*100)\n",
    "\n",
    "## As per the logic to tag churners based on the 9th month's behaviour, i.e. no call and no usage\n",
    "## only around 10% of customers are there who have churned which makes this problem highly imbalanced in \n",
    "## nature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_tag=pd.merge(inp_fl,churn_check[['mobile_number','churn']],on='mobile_number',how='inner')\n",
    "inp_tag.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f1dd16",
   "metadata": {},
   "source": [
    "### Building the data set for EDA and Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f3ac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we have to split the data based on columns , such that churn behaviour is trained on columns ,6,7,8 and 9th is \n",
    "## used for the prediction purpose\n",
    "all_columns=inp_tag.columns\n",
    "all_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc45f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "month9_cols=[col for col in all_columns if '_9' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2a0324",
   "metadata": {},
   "outputs": [],
   "source": [
    "## rest of the columns\n",
    "rest_columns=[col for col in all_columns if col not in month9_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec934577",
   "metadata": {},
   "outputs": [],
   "source": [
    "## base data preparation\n",
    "base_df=inp_tag[rest_columns]\n",
    "base_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a07218",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if customers belonging to particular circles \n",
    "base_df.circle_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ef7fe6",
   "metadata": {},
   "source": [
    "#### Now understanding Usage, recharge, functionality etc, which will describe how the customer behaviour changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f64b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Understanding the recharge schedule and making features of the recharge. \n",
    "## Assumption is if recharge amount is decreasing or diminishing then that can be a signal towards churning\n",
    "\n",
    "recharge_cols=[col for col in rest_columns if 'rech' in col]\n",
    "recharge_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d89b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "recharge_df=base_df[['mobile_number']+recharge_cols+['churn']]\n",
    "recharge_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf205c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can keep overall recharge as one and data recharge as just a boost to base recharge \n",
    "recharge_df['total_rech_data_6'].fillna(0,inplace=True)\n",
    "recharge_df['total_rech_data_7'].fillna(0,inplace=True)\n",
    "recharge_df['total_rech_data_8'].fillna(0,inplace=True)\n",
    "\n",
    "recharge_df['av_rech_amt_data_6'].fillna(0,inplace=True)\n",
    "recharge_df['av_rech_amt_data_7'].fillna(0,inplace=True)\n",
    "recharge_df['av_rech_amt_data_8'].fillna(0,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd7d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "recharge_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e46b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only using recharge and data information to understand the behaviour\n",
    "## drop columns with NAs in it\n",
    "recharge_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instead of Dates we can change it to days since the last recharge and average days inbetween the recharges. \n",
    "recharge_df[recharge_df['date_of_last_rech_6'].isna()].head()\n",
    "\n",
    "## It can be seen that people either entered into the system after 6th month and maybe they were there in the system \n",
    "## but left the system before the 9th month (assuming both 7th and 8th months has no recharge in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e40da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check when it has both 3g and 2g recharges \n",
    "recharge_df['recharge_type_data_6_both']=np.where((recharge_df['count_rech_2g_6']>0) & \n",
    "                                                  (recharge_df['count_rech_3g_6']>0),1,0)\n",
    "recharge_df['recharge_type_data_7_both']=np.where((recharge_df['count_rech_2g_7']>0) & \n",
    "                                                  (recharge_df['count_rech_3g_7']>0),1,0)\n",
    "recharge_df['recharge_type_data_8_both']=np.where((recharge_df['count_rech_2g_8']>0) & \n",
    "                                                  (recharge_df['count_rech_3g_8']>0),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ca375",
   "metadata": {},
   "outputs": [],
   "source": [
    "recharge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8505106",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Date transformation \n",
    "# Custom function to calculate count based on NA values for the subset of columns\n",
    "# Assuming you have a DataFrame called 'df' with three columns 'col1', 'col2', and 'col3'\n",
    "subset_columns = ['date_of_last_rech_6', 'date_of_last_rech_7', 'date_of_last_rech_8']\n",
    "def count_based_on_na(row):\n",
    "    num_nas = row[subset_columns].isnull().sum()\n",
    "    if num_nas == 0:\n",
    "        return 3\n",
    "    elif num_nas == 1:\n",
    "        return 2\n",
    "    elif num_nas == 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the custom function row-wise to calculate the count for each row in the subset of columns\n",
    "\n",
    "recharge_df['rech_month_active']=recharge_df.apply(count_based_on_na, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eea23c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change recharge dates to 0 and 1 , as categorical \n",
    "recharge_df['date_of_last_rech_6']= np.where(recharge_df['date_of_last_rech_6'].isna(),0,1)\n",
    "recharge_df['date_of_last_rech_7']=np.where(recharge_df['date_of_last_rech_7'].isna(),0,1)\n",
    "recharge_df['date_of_last_rech_8']=np.where(recharge_df['date_of_last_rech_8'].isna(),0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5807d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "recharge_df.head()\n",
    "# Drop columns with NA (missing values)\n",
    "recharge_df = recharge_df.dropna(axis=1)\n",
    "recharge_df.head()\n",
    "\n",
    "## Idea is , if mobile is getting recharged and it has any data pack 3g or 2g as recharged in it, then the \n",
    "## recharge count of data and its amounts will impact, either of the data type will not impact, but person\n",
    "## recharging both data packs might have different behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9127fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting slope and change in recharge amounts, to understand if that impact of M-O-M change has on churn\n",
    "recharge_df['rech_amt_7n6_slope']=recharge_df['total_rech_amt_7']-recharge_df['total_rech_amt_6']\n",
    "recharge_df['rech_amt_8n7_slope']=recharge_df['total_rech_amt_8']-recharge_df['total_rech_amt_7']\n",
    "recharge_df['rech_slope_tag']=np.where((recharge_df['rech_amt_7n6_slope']<=0)&\n",
    "                                       (recharge_df['rech_amt_8n7_slope']<=0),-1,\n",
    "                               np.where((recharge_df['rech_amt_7n6_slope']>0)&\n",
    "                                       (recharge_df['rech_amt_8n7_slope']>0),1,0))\n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ae974",
   "metadata": {},
   "outputs": [],
   "source": [
    "recharge_df['avg_rech_amt']=(recharge_df['total_rech_amt_6']+recharge_df['total_rech_amt_7']+recharge_df['total_rech_amt_8'])/3\n",
    "recharge_df.head() \n",
    "\n",
    "## recharge data is completed, with average recharge and the kind of data recharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b91a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.isna().sum()/len(base_df)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ff8395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the percentage of null values in each column\n",
    "def calculate_null_percentage(column):\n",
    "    total_rows = len(base_df)\n",
    "    null_count = column.isnull().sum()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    return null_percentage\n",
    "\n",
    "# Calculate the threshold percentage (e.g., 30%)\n",
    "threshold_percentage = 50\n",
    "\n",
    "# Drop columns with null values greater than the threshold percentage of 50%\n",
    "base_df_filtered = base_df.drop(base_df.columns[base_df.apply(calculate_null_percentage) > threshold_percentage], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1dcd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c10487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the column names and their unique values\n",
    "columns_with_unique_values = {}\n",
    "\n",
    "# Loop through each column\n",
    "for column in base_df_filtered.columns:\n",
    "    unique_values = base_df_filtered[column].unique()\n",
    "    num_unique_values = base_df_filtered[column].nunique()\n",
    "    \n",
    "    # Check if the number of unique values is less than 10\n",
    "    if num_unique_values < 5:\n",
    "        columns_with_unique_values[column] = {'Num_Unique_Values': num_unique_values, 'Unique_Values': unique_values}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14ef730",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_unique_values\n",
    "## It can be seen that these columns does not have much information in it, so we can remove these columns too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14de76c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_remove=list(columns_with_unique_values.keys())\n",
    "columns_remove=columns_remove[:len(columns_remove)-1]\n",
    "columns_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf80a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df_filtered.drop(columns=columns_remove,inplace=True)\n",
    "base_df_filtered.shape\n",
    "\n",
    "## Now we have 124 columns, we can remove recharge ones from this too "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d3366",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df_filtered=base_df_filtered[[col for col in base_df_filtered.columns if col not in recharge_cols]]\n",
    "base_df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a43de",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df_filtered.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb89007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_df_filtered.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf12a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Onnet VS Offnet usage, -if user has more out of network calls or usage , does that impact the churn rate \n",
    "## Given this T2T, T2M, T2o etc will be covered as part of it so we can remove all those columns and just focust on \n",
    "## onnet vs offnet \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f212ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_highly_correlated_columns(df, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Function to find column pairs with correlation greater than a specified threshold.\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas.DataFrame): The DataFrame containing the data.\n",
    "        threshold (float, optional): The correlation threshold. Defaults to 0.8.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing correlated column pairs and their correlation values.\n",
    "    \"\"\"\n",
    "    # Compute the correlation matrix\n",
    "    correlation_matrix = df.corr()\n",
    "\n",
    "    # Find pairs of columns with correlation greater than the threshold\n",
    "    correlated_column_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "                col_pair = (correlation_matrix.columns[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j])\n",
    "                correlated_column_pairs.append(col_pair)\n",
    "\n",
    "    # Create a DataFrame to store the correlated column pairs and their correlation values\n",
    "    df_correlated = pd.DataFrame(correlated_column_pairs, columns=['Column1', 'Column2', 'Correlation'])\n",
    "\n",
    "    return df_correlated\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have a DataFrame called 'df'\n",
    "correlated_columns = get_highly_correlated_columns(base_df_filtered, threshold=0.75)\n",
    "print(correlated_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e802fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59233d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can remove the columns of sachet and rch. as recharge columns we already have \n",
    "drop_cols2=['last_day_rch_amt_6',\n",
    "       'last_day_rch_amt_7', 'last_day_rch_amt_8']\n",
    "base_df_filtered.drop(columns=drop_cols2,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011fae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df_filtered.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4911d7d2",
   "metadata": {},
   "source": [
    "#### How Churn is impacted by multiple scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e5bdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "recharge_df.drop(columns='churn',inplace=True)\n",
    "\n",
    "base_data_all=pd.merge(base_df_filtered,recharge_df,on='mobile_number',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4185a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_data_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab9a86a",
   "metadata": {},
   "source": [
    "### Filtering 70th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c96705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for missing value and treatment in the high value customers, lets create the dataset first\n",
    "#Deriving Average recharge amount of June and July.\n",
    "base_data_all['average_rech_amt_6n7']=(base_data_all['total_rech_amt_6']+base_data_all['total_rech_amt_7'])/2\n",
    "\n",
    "#Filtering based HIGH VALUED CUSTOMERS based on (Average_rech_amt_6n7 >= 70th percentile of Average_rech_amt_6n7)\n",
    "base_data_high_val=base_data_all[(base_data_all['average_rech_amt_6n7']>= base_data_all['average_rech_amt_6n7'].quantile(0.7))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21347fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_high_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edcafa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check missing value by each month columns \n",
    "base_data_high_val.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec7ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_val_data_cols=list(base_data_high_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Missing values in 6th months\n",
    "month6_data=base_data_high_val[[col for col in high_val_data_cols if '_6' in col]]\n",
    "month6_data.isna().sum()/len(month6_data)*100\n",
    "\n",
    "## it can be seen that , all the MOU columns have a similar % of missing value in it, which can be related to \n",
    "## some pattern with the users, as this cannot be random. If they are not using calling plans then their total og\n",
    "## and ic should also be zero, which we can confirm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e6e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "month6_data[month6_data['onnet_mou_6'].isna()]['total_ic_mou_6'].unique()\n",
    "## Their total mou is also zero, which means they are not using any call packs, we can fill these NA values with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432e3452",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_high_val[list(month6_data.columns)]=base_data_high_val[list(month6_data.columns)].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe07016",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now similarly we can check for month 7 \n",
    "month7_data=base_data_high_val[[col for col in high_val_data_cols if '_7' in col]]\n",
    "month7_data.isna().sum()/len(month6_data)*100\n",
    "## It has similar chance, we can fill it again with 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc4ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_high_val[list(month7_data.columns)]=base_data_high_val[list(month7_data.columns)].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0447362",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now similarly we can check for month 8\n",
    "month8_data=base_data_high_val[[col for col in high_val_data_cols if '_8' in col]]\n",
    "month8_data.isna().sum()/len(month8_data)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c2be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_high_val[list(month8_data.columns)]=base_data_high_val[list(month8_data.columns)].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_high_val.isna().any()\n",
    "## Now there are no missing values in this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeec4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking churn with change average revenue per use \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "arpu_data=base_data_high_val.groupby(['churn'])['arpu_6','arpu_7','arpu_8'].agg(np.median).reset_index()\n",
    "arpu_data_melt = arpu_data.melt(id_vars='churn',value_vars=['arpu_6','arpu_7','arpu_8'], var_name='Columns', value_name='Values')\n",
    "\n",
    "sns.barplot(x='Columns', y='Values',hue='churn', data=arpu_data_melt)\n",
    "plt.title('Median Revenue by Churn')\n",
    "plt.show()\n",
    "\n",
    "## People having higher chance of churn can see a dip in ARPU which is significant enough from, we can now get derived \n",
    "## metrics to mark the slope by users\n",
    "base_data_high_val['arpu_slope_7n6']=base_data_high_val['arpu_7']-base_data_high_val['arpu_6']\n",
    "base_data_high_val['arpu_slope_8n7']=base_data_high_val['arpu_8']-base_data_high_val['arpu_7']\n",
    "base_data_high_val['arpu_slope_tag']=np.where((base_data_high_val['arpu_slope_7n6']<=0)&\n",
    "                                       (base_data_high_val['arpu_slope_8n7']<=0),-1,\n",
    "                               np.where((base_data_high_val['arpu_slope_7n6']>0)&\n",
    "                                       (base_data_high_val['arpu_slope_8n7']>0),1,0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766f8d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking onnet vs offnet against the churn\n",
    "## if user has higher offnet \n",
    "arpu_data=base_data_high_val.groupby(['churn'])['onnet_mou_6','onnet_mou_7','onnet_mou_8','offnet_mou_6','offnet_mou_7','offnet_mou_8'].agg(np.mean).reset_index()\n",
    "arpu_data_melt = arpu_data.melt(id_vars='churn',value_vars=['onnet_mou_6','onnet_mou_7','onnet_mou_8',\n",
    "                                                           'offnet_mou_6','offnet_mou_7','offnet_mou_8'], var_name='Columns', value_name='Values')\n",
    "\n",
    "sns.barplot(x='Columns', y='Values',hue='churn', data=arpu_data_melt)\n",
    "plt.title('Network type by Churn')\n",
    "plt.show()\n",
    "\n",
    "# ## People having higher chance of churn by reduction in onnet as well as offnet\n",
    "## let take the ratio of offnet by onnet if there is usage distirbution impacting it \n",
    "base_data_high_val['on_off_ratio_6']=np.where((base_data_high_val['onnet_mou_6']==0) | \n",
    "                                         (base_data_high_val['offnet_mou_6']==0),0,\n",
    "                                          base_data_high_val['onnet_mou_6']/(base_data_high_val['onnet_mou_6']+base_data_high_val['onnet_mou_6']))\n",
    "\n",
    "base_data_high_val['on_off_ratio_7']=np.where((base_data_high_val['onnet_mou_7']==0) | \n",
    "                                         (base_data_high_val['offnet_mou_7']==0),0,\n",
    "                                          base_data_high_val['onnet_mou_7']/(base_data_high_val['onnet_mou_7']+base_data_high_val['onnet_mou_7']))\n",
    "\n",
    "base_data_high_val['on_off_ratio_8']=np.where((base_data_high_val['onnet_mou_8']==0) | \n",
    "                                         (base_data_high_val['offnet_mou_8']==0),0,\n",
    "                                          base_data_high_val['onnet_mou_8']/(base_data_high_val['onnet_mou_8']+base_data_high_val['onnet_mou_8']))\n",
    "\n",
    "\n",
    "arpu_data=base_data_high_val.groupby(['churn'])['on_off_ratio_6','on_off_ratio_7','on_off_ratio_8'].agg(np.mean).reset_index()\n",
    "arpu_data_melt = arpu_data.melt(id_vars='churn',value_vars=['on_off_ratio_6','on_off_ratio_7','on_off_ratio_8'],\n",
    "                                var_name='Columns', value_name='Values')\n",
    "\n",
    "sns.barplot(x='Columns', y='Values',hue='churn', data=arpu_data_melt)\n",
    "plt.title('Onnet-Offnet ratio by Churn')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## However if we see from a onnet to offnet ratio, the ones churning are basically due to decrease in onnet usage\n",
    "## so if the one reducing the local usage has higher chance to churn. People not churning has around 45/55 split of \n",
    "## onnet vs offnet usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6043c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_high_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d792b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_data_high_val['onnet_offnet_slope'] = (base_data_high_val['on_off_ratio_8']-base_data_high_val['on_off_ratio_6']).add(base_data_high_val['on_off_ratio_7']).div(2)\n",
    "base_data_high_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ee63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Understand how we are getting impact by high revenue customer and low value customer segement \n",
    "## High valuable customers can be described by arpu and aon of the customer \n",
    "## We can create divide the customers by deciles to understand how much percentage of customers are getting \n",
    "## churned in which deciles \n",
    "\n",
    "## Deciling by network age time and ARPU \n",
    "\n",
    "## Understanding the time concept \n",
    "# Create a histogram with hue using displot\n",
    "plt.figure(figsize=(10,20))\n",
    "sns.displot(data=base_data_high_val, x='aon', hue='churn', kde=True, multiple='stack', palette='colorblind')\n",
    "plt.title('Churn distribution by customer time')\n",
    "## Both churn vs non churn customers have skewed distribution, but it can be seen that people spedning more than \n",
    "## 1000 days with the operator has lesser chance to churn as most of the churn is happening with the new customers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d47f9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "base_data_high_val[base_data_high_val['arpu_6']<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dfb187",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can combine roaming as one , idea is if person uses either IC or OG roaming in any month then it has roaming\n",
    "## active for that month, also the roaming minutes will directly impact the churn too, hypothesis is \n",
    "## if my network is benificial with roaming then high MOU customer will stick the network\n",
    "all_columns=list(base_data_high_val.columns)\n",
    "\n",
    "roam_cols=[col for col in all_columns if 'roam' in col]\n",
    "print(roam_cols)\n",
    "\n",
    "local_cols=[col for col in all_columns if 'loc' in col]\n",
    "print(local_cols)\n",
    "\n",
    "special_cols=[col for col in all_columns if 'spl' in col]\n",
    "print(special_cols)\n",
    "\n",
    "isd_cols=[col for col in all_columns if 'isd' in col]\n",
    "print(isd_cols)\n",
    "\n",
    "std_cols=[col for col in all_columns if 'std' in col]\n",
    "print(std_cols)\n",
    "\n",
    "other_cols=[col for col in all_columns if 'other' in col]\n",
    "print(other_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e3a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_df=base_data_high_val[['mobile_number']+local_cols+['churn']]\n",
    "\n",
    "## Checking with overall total local = t2t+t2m+t2f+t2c, this change in % in these behaviours can impact \n",
    "## how the churn eventually happens \n",
    "\n",
    "## Changing everything to percentage rather than the actual numbers \n",
    "for col in local_cols:\n",
    "    if '6' in col:\n",
    "        if 'loc_og' in col:\n",
    "            local_df[col]=round(local_df[col]/local_df['loc_og_mou_6']*100)\n",
    "        else:\n",
    "            local_df[col]=round(local_df[col]/local_df['loc_ic_mou_6']*100)\n",
    "    elif '7' in col:\n",
    "        if 'loc_og' in col :\n",
    "            local_df[col]=round(local_df[col]/local_df['loc_og_mou_7']*100)\n",
    "        else:\n",
    "            local_df[col]=round(local_df[col]/local_df['loc_ic_mou_7']*100)\n",
    "    else:\n",
    "        if 'loc_og' in col:\n",
    "            local_df[col]=round(local_df[col]/local_df['loc_og_mou_8']*100)\n",
    "        else:\n",
    "            local_df[col]=round(local_df[col]/local_df['loc_ic_mou_8']*100)\n",
    "        \n",
    "        \n",
    "            \n",
    "local_df.fillna(0,inplace=True)\n",
    "local_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_df.drop(columns=['loc_og_mou_6', 'loc_og_mou_7', 'loc_og_mou_8','loc_ic_mou_6', 'loc_ic_mou_7', 'loc_ic_mou_8'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315df8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_df.columns[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161d185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How the churn changes with change in local usage behaviour\n",
    "local_df_melt=local_df.melt(id_vars=['mobile_number','churn'],value_vars=list(local_df.columns[1:-1]),\n",
    "                           var_name='Tag',value_name='Values')\n",
    "local_df_melt['month']=local_df_melt['Tag'].apply(lambda x: x.split('_')[-1])\n",
    "local_df_melt['ic_og']=local_df_melt['Tag'].apply(lambda x: x.split('_')[1])\n",
    "local_df_melt['call_type']=local_df_melt['Tag'].apply(lambda x: x.split('_')[2])\n",
    "local_df_melt.drop(columns='Tag',inplace=True)\n",
    "local_df_melt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dbf5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "local_df_melt.groupby(['ic_og','month','call_type','churn'])['Values'].agg(np.mean).plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44dac15",
   "metadata": {},
   "source": [
    "The ones churning have a significant reduction in t2m in the 8th month, meaning user signiicantly stopped calling to other operators using our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47daaf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "arpu_data_melt = arpu_data.melt(id_vars='churn',value_vars=['on_off_ratio_6','on_off_ratio_7','on_off_ratio_8'],\n",
    "                                var_name='Columns', value_name='Values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d5a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_df=base_data_high_val[['mobile_number']+std_cols+['churn']]\n",
    "\n",
    "## Checking with overall total std = t2t+t2m+t2f+t2c, this change in % in these behaviours can impact \n",
    "## how the churn eventually happens \n",
    "\n",
    "## Changing everything to percentage rather than the actual numbers \n",
    "for col in std_cols:\n",
    "    if '6' in col:\n",
    "        if 'std_og' in col:\n",
    "            std_df[col]=round(std_df[col]/std_df['std_og_mou_6']*100)\n",
    "        else:\n",
    "            std_df[col]=round(std_df[col]/std_df['std_ic_mou_6']*100)\n",
    "    elif '7' in col:\n",
    "        if 'std_og' in col :\n",
    "            std_df[col]=round(std_df[col]/std_df['std_og_mou_7']*100)\n",
    "        else:\n",
    "            std_df[col]=round(std_df[col]/std_df['std_ic_mou_7']*100)\n",
    "    else:\n",
    "        if 'std_og' in col:\n",
    "            std_df[col]=round(std_df[col]/std_df['std_og_mou_8']*100)\n",
    "        else:\n",
    "            std_df[col]=round(std_df[col]/std_df['std_ic_mou_8']*100)\n",
    "        \n",
    "        \n",
    "            \n",
    "std_df.fillna(0,inplace=True)\n",
    "std_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63882b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_df.drop(columns=['std_og_mou_6', 'std_og_mou_7', 'std_og_mou_8','std_ic_mou_6', 'std_ic_mou_7', 'std_ic_mou_8'],inplace=True)\n",
    "\n",
    "std_df.columns[1:-1]\n",
    "\n",
    "## How the churn changes with change in std usage behaviour\n",
    "std_df_melt=std_df.melt(id_vars=['mobile_number','churn'],value_vars=list(std_df.columns[1:-1]),\n",
    "                           var_name='Tag',value_name='Values')\n",
    "std_df_melt['month']=std_df_melt['Tag'].apply(lambda x: x.split('_')[-1])\n",
    "std_df_melt['ic_og']=std_df_melt['Tag'].apply(lambda x: x.split('_')[1])\n",
    "std_df_melt['call_type']=std_df_melt['Tag'].apply(lambda x: x.split('_')[2])\n",
    "std_df_melt.drop(columns='Tag',inplace=True)\n",
    "std_df_melt.head()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "std_df_melt.groupby(['ic_og','month','call_type','churn'])['Values'].agg(np.mean).plot(kind='bar')\n",
    "\n",
    "## STD customers calling in the same network in the good phase are churning out more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff614098",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking if roaming and std has high correlation, and we can remove roaming from it \n",
    "# Assuming you have a DataFrame called 'df'\n",
    "\n",
    "roam_std_df=base_data_high_val[['mobile_number']+std_cols+roam_cols+['churn']]\n",
    "correlated_columns = get_highly_correlated_columns(roam_std_df, threshold=0.7)\n",
    "print(correlated_columns)\n",
    "\n",
    "## roaming has no relation with STD, so we can check the impact of Roaming on the churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking if roaming and std has \n",
    "\n",
    "roam_df=base_data_high_val[['mobile_number']+roam_cols+['churn']]\n",
    "\n",
    "roam_df['roam_6_total']=roam_df['roam_ic_mou_6']+roam_df['roam_og_mou_6']\n",
    "roam_df['roam_7_total']=roam_df['roam_ic_mou_7']+roam_df['roam_og_mou_7']\n",
    "roam_df['roam_8_total']=roam_df['roam_ic_mou_8']+roam_df['roam_og_mou_8']\n",
    "roam_df['roam_tag']=np.where((roam_df['roam_6_total']==0) & (roam_df['roam_7_total']==0)& (roam_df['roam_8_total']==0),\n",
    "                            'No Roaming',np.where((roam_df['roam_6_total']>0) | (roam_df['roam_7_total']>0) &\n",
    "                                                 (roam_df['roam_8_total']==0),'Good Phase Roaming',\n",
    "                            np.where((roam_df['roam_6_total']==0) & (roam_df['roam_7_total']==0)& (roam_df['roam_8_total']>0),\n",
    "                                    'Action Phase Roaming', 'All Time Roaming')) )\n",
    "\n",
    "print('Roaming Active all time')\n",
    "print(roam_df[roam_df['roam_tag']=='All Time Roaming']['churn'].value_counts()/len(roam_df[roam_df['roam_tag']=='All Time Roaming']))\n",
    "\n",
    "print('Roaming not active')\n",
    "print(roam_df[roam_df['roam_tag']=='No Roaming']['churn'].value_counts()/len(roam_df[roam_df['roam_tag']=='No Roaming']))\n",
    "\n",
    "print('Only Good Phase active')\n",
    "print(roam_df[roam_df['roam_tag']=='Good Phase Roaming']['churn'].value_counts()/len(roam_df[roam_df['roam_tag']=='Good Phase Roaming']))\n",
    "\n",
    "print('Only Action Phase Active active')\n",
    "print(roam_df[roam_df['roam_tag']=='Action Phase Roaming']['churn'].value_counts()/len(roam_df[roam_df['roam_tag']=='Action Phase Roaming']))\n",
    "\n",
    "\n",
    "\n",
    "## users actively using roaming , even for once in a 3 month time has higher chance to churn against people\n",
    "## who never used roaming in these 3 months, All time active roamers along with action phase roamers, ~25% of those\n",
    "## users are leaving the operator, so somewhere roaming plans are not as lucrative as can be seen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cc896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## No checking plans of special uses and its impact on the churn value \n",
    "special_df=base_data_high_val[['mobile_number']+special_cols+['churn']]\n",
    "\n",
    "special_df['special_6_total']=special_df['spl_ic_mou_6']+special_df['spl_og_mou_6']\n",
    "special_df['special_7_total']=special_df['spl_ic_mou_7']+special_df['spl_og_mou_7']\n",
    "special_df['special_8_total']=special_df['spl_ic_mou_8']+special_df['spl_og_mou_8']\n",
    "special_df['special_tag']=np.where((special_df['special_6_total']==0) & (special_df['special_7_total']==0)& (special_df['special_8_total']==0),\n",
    "                            'No Special Use',np.where((special_df['special_6_total']>0) | (special_df['special_7_total']>0) &\n",
    "                                                 (special_df['special_8_total']==0),'Good Phase Special Use',\n",
    "                            np.where((special_df['special_6_total']==0) & (special_df['special_7_total']==0)& (special_df['special_8_total']>0),\n",
    "                                    'Action Phase Special Use', 'All Time Special Use')) )\n",
    "\n",
    "print('Special Use Active all time')\n",
    "print(special_df[special_df['special_tag']=='All Time Special Use']['churn'].value_counts()/len(special_df[special_df['special_tag']=='All Time Special Use']))\n",
    "\n",
    "print('Special Use not active')\n",
    "print(special_df[special_df['special_tag']=='No Special Use']['churn'].value_counts()/len(special_df[special_df['special_tag']=='No Special Use']))\n",
    "\n",
    "print('Only Good Phase active')\n",
    "print(special_df[special_df['special_tag']=='Good Phase Special Use']['churn'].value_counts()/len(special_df[special_df['special_tag']=='Good Phase Special Use']))\n",
    "\n",
    "print('Only Action Phase Active')\n",
    "print(special_df[special_df['special_tag']=='Action Phase Special Use']['churn'].value_counts()/len(special_df[special_df['special_tag']=='Action Phase Special Use']))\n",
    "\n",
    "## Special usage has better chance of not churning though, especially in the action phase against the ones who\n",
    "## never used a special pack, we we can see the customer not using a special pack, we can recommend that \n",
    "## in their action phase to reduce the churn from 12% to 2% from these figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7153d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking plans of isd uses and its impact on the churn value \n",
    "isd_df=base_data_high_val[['mobile_number']+isd_cols+['churn']]\n",
    "\n",
    "isd_df['isd_6_total']=isd_df['isd_ic_mou_6']+isd_df['isd_og_mou_6']\n",
    "isd_df['isd_7_total']=isd_df['isd_ic_mou_7']+isd_df['isd_og_mou_7']\n",
    "isd_df['isd_8_total']=isd_df['isd_ic_mou_8']+isd_df['isd_og_mou_8']\n",
    "isd_df['isd_tag']=np.where((isd_df['isd_6_total']==0) & (isd_df['isd_7_total']==0)& (isd_df['isd_8_total']==0),\n",
    "                            'No isd Use',np.where((isd_df['isd_6_total']>0) | (isd_df['isd_7_total']>0) &\n",
    "                                                 (isd_df['isd_8_total']==0),'Good Phase isd Use',\n",
    "                            np.where((isd_df['isd_6_total']==0) & (isd_df['isd_7_total']==0)& (isd_df['isd_8_total']>0),\n",
    "                                    'Action Phase isd Use', 'All Time isd Use')) )\n",
    "\n",
    "print('isd Use Active all time')\n",
    "print(isd_df[isd_df['isd_tag']=='All Time isd Use']['churn'].value_counts()/len(isd_df[isd_df['isd_tag']=='All Time isd Use']))\n",
    "\n",
    "print('isd Use not active')\n",
    "print(isd_df[isd_df['isd_tag']=='No isd Use']['churn'].value_counts()/len(isd_df[isd_df['isd_tag']=='No isd Use']))\n",
    "\n",
    "print('Only Good Phase active')\n",
    "print(isd_df[isd_df['isd_tag']=='Good Phase isd Use']['churn'].value_counts()/len(isd_df[isd_df['isd_tag']=='Good Phase isd Use']))\n",
    "\n",
    "print('Only Action Phase Active')\n",
    "print(isd_df[isd_df['isd_tag']=='Action Phase isd Use']['churn'].value_counts()/len(isd_df[isd_df['isd_tag']=='Action Phase isd Use']))\n",
    "\n",
    "\n",
    "## ISD usage does not significant change in Usage vs Non-usage, so it is not directly changing the churn rate, \n",
    "## but still action phase has improve the churn as compared to no-usage at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae68faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "other_df=base_data_high_val[['mobile_number']+other_cols+['churn']]\n",
    "\n",
    "other_df['other_6_total']=other_df['ic_others_6']+other_df['og_others_6']\n",
    "other_df['other_7_total']=other_df['ic_others_7']+other_df['og_others_7']\n",
    "other_df['other_8_total']=other_df['ic_others_8']+other_df['og_others_8']\n",
    "other_df['other_tag']=np.where((other_df['other_6_total']==0) & (other_df['other_7_total']==0)& (other_df['other_8_total']==0),\n",
    "                            'No other Use',np.where((other_df['other_6_total']>0) | (other_df['other_7_total']>0) &\n",
    "                                                 (other_df['other_8_total']==0),'Good Phase other Use',\n",
    "                            np.where((other_df['other_6_total']==0) & (other_df['other_7_total']==0)& (other_df['other_8_total']>0),\n",
    "                                    'Action Phase other Use', 'All Time other Use')) )\n",
    "\n",
    "print('other Use Active all time')\n",
    "print(other_df[other_df['other_tag']=='All Time other Use']['churn'].value_counts()/len(other_df[other_df['other_tag']=='All Time other Use']))\n",
    "\n",
    "print('other Use not active')\n",
    "print(other_df[other_df['other_tag']=='No other Use']['churn'].value_counts()/len(other_df[other_df['other_tag']=='No other Use']))\n",
    "\n",
    "print('Only Good Phase active')\n",
    "print(other_df[other_df['other_tag']=='Good Phase other Use']['churn'].value_counts()/len(other_df[other_df['other_tag']=='Good Phase other Use']))\n",
    "\n",
    "print('Only Action Phase Active')\n",
    "print(other_df[other_df['other_tag']=='Action Phase other Use']['churn'].value_counts()/len(other_df[other_df['other_tag']=='Action Phase other Use']))\n",
    "\n",
    "\n",
    "## one interesting to watch here is others and isd service numbers are almost similar , probably they are closely \n",
    "## related and ISD might be a part of other services provided, we can check the correlation between them and remove\n",
    "## if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c50ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## correlation between ISD and other services \n",
    "others_isd_df=pd.merge(isd_df,other_df,on=['mobile_number','churn'],how='inner')\n",
    "correlated_columns = get_highly_correlated_columns(others_isd_df, threshold=0.7)\n",
    "print(correlated_columns)\n",
    "\n",
    "## ISD is not a related to other services, but still it has a similar impact on the set of users which either used \n",
    "## ISD or others \n",
    "print('    ')\n",
    "print('other  & ISD Use Active all time')\n",
    "print(others_isd_df[(others_isd_df['other_tag']=='All Time other Use') & \n",
    "                   (others_isd_df['isd_tag']=='All Time isd Use')]['churn'].value_counts())\n",
    "\n",
    "## There are only 72 users which have used ISD and other services together, and around 5% of them only churned\n",
    "## which is small number as compared to other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc3605",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking the impact of data pack usage and its impact on the churn rates\n",
    "base_data_high_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce944474",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2g_cols=[col for col in all_columns if '2g' in col]\n",
    "data_3g_cols=[col for col in all_columns if '3g' in col]\n",
    "data_cols=data_2g_cols+data_3g_cols\n",
    "data_cols=data_cols[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfebfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first we have to understand if people use 2g and 3g together \n",
    "data_usage_df=base_data_high_val[['mobile_number']+data_cols+['churn']]\n",
    "data_usage_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f40999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage \n",
    "data_usage_df['delta_vol_2g'] = data_usage_df['vol_2g_mb_8'] - data_usage_df['vol_2g_mb_6'].add(data_usage_df['vol_2g_mb_7']).div(2)\n",
    "data_usage_df['delta_vol_3g'] = data_usage_df['vol_3g_mb_8'] - data_usage_df['vol_3g_mb_6'].add(data_usage_df['vol_3g_mb_7']).div(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625bdaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check the change in data usage by churn \n",
    "data_usage_df['data_tag']=np.where((data_usage_df['delta_vol_2g']<0)&(data_usage_df['delta_vol_3g']>0),\n",
    "                                  '3g_transition',\n",
    "                           np.where((data_usage_df['delta_vol_2g']>0)&(data_usage_df['delta_vol_3g']<0),\n",
    "                                   '2g_transition',\n",
    "                           np.where((data_usage_df['delta_vol_2g']==0)&(data_usage_df['delta_vol_3g']>0),\n",
    "                                   '3g_user_positive',\n",
    "                           np.where((data_usage_df['delta_vol_2g']>0)&(data_usage_df['delta_vol_3g']==0),\n",
    "                                    '2g_user_positive',\n",
    "                           np.where((data_usage_df['delta_vol_2g']>0)&(data_usage_df['delta_vol_3g']>0),\n",
    "                                    'data_lover',\n",
    "                           np.where(((data_usage_df['delta_vol_2g']<0) & (data_usage_df['delta_vol_3g']==0)) |\n",
    "                                    ((data_usage_df['delta_vol_2g']==0) & (data_usage_df['delta_vol_3g']<0)),\n",
    "                                    'data_use_reducer','data_non_lover'\n",
    "                                   ))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b4c9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_usage_grouped=data_usage_df.groupby(['data_tag','churn'])['mobile_number'].count().reset_index()\n",
    "data_usage_grouped['total']=data_usage_grouped.groupby(['data_tag'])['mobile_number'].transform(np.sum)\n",
    "data_usage_grouped['percent']=data_usage_grouped['mobile_number']/data_usage_grouped['total']\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='data_tag',y='percent',data=data_usage_grouped,hue='churn')\n",
    "plt.show()\n",
    "\n",
    "## People who do love using data, are not moving out , ones who have not used the data at all are churning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45afad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sachet  vs sachet VS vbc usage of data \n",
    "data_usage_df['vbc_delta']=data_usage_df['aug_vbc_3g'] - data_usage_df['jun_vbc_3g'].add(data_usage_df['jul_vbc_3g']).div(2)\n",
    "data_usage_df['sachet_delta_2g']=data_usage_df['sachet_2g_8'] - data_usage_df['sachet_2g_6'].add(data_usage_df['sachet_2g_7']).div(2)\n",
    "data_usage_df['sachet_delta_3g']=data_usage_df['sachet_3g_8'] - data_usage_df['sachet_3g_8'].add(data_usage_df['sachet_3g_8']).div(2)\n",
    "\n",
    "data_usage_df['sachet_delta_2g']=data_usage_df['sachet_2g_8'] - data_usage_df['sachet_2g_6'].add(data_usage_df['sachet_2g_7']).div(2)\n",
    "data_usage_df['sachet_delta_3g']=data_usage_df['sachet_3g_8'] - data_usage_df['sachet_3g_8'].add(data_usage_df['sachet_3g_8']).div(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=data_usage_df,x='vbc_delta',y='sachet_delta_2g',hue='churn')\n",
    "plt.title('VBC usage versus 2g sachet recharge')\n",
    "plt.show()\n",
    "## Churn and reduction in data usage of 2g sachets and VBC are related\n",
    "\n",
    "sns.scatterplot(data=data_usage_df,x='vbc_delta',y='sachet_delta_3g',hue='churn')\n",
    "plt.title('VBC usage versus 3g sachet recharge')\n",
    "plt.show()\n",
    "## If a person with no change in 3g sachets but only dependent on VBC has impact on churn, 3g has no significance \n",
    "## in the non-churning \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5319df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## making the final analytical data with the derived columns \n",
    "local_df.columns=[col+'_perc' if col not in ['mobile_number','churn'] else col for col in list(local_df.columns)]\n",
    "local_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### std df\n",
    "std_df.columns=[col+'_perc' if col not in ['mobile_number','churn'] else col for col in list(std_df.columns)]\n",
    "std_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c765489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_df.drop(columns='churn',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765fbbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_df.drop(columns='churn',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2260489",
   "metadata": {},
   "outputs": [],
   "source": [
    "roam_df=roam_df[['mobile_number','roam_6_total', 'roam_7_total', 'roam_8_total', 'roam_tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c51a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "isd_df=isd_df[['mobile_number','isd_6_total', 'isd_7_total', 'isd_8_total', 'isd_tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d98efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_df=other_df[['mobile_number','other_6_total', 'other_7_total', 'other_8_total', 'other_tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657a08b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_df=special_df[['mobile_number','special_6_total', 'special_7_total', 'special_8_total', 'special_tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98341b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_usage_df=data_usage_df[['mobile_number','delta_vol_2g', 'delta_vol_3g', 'data_tag',\n",
    "       'vbc_delta', 'sachet_delta_2g', 'sachet_delta_3g']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939373d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# Assuming you have multiple DataFrames called df1, df2, df3, etc. that you want to merge\n",
    "\n",
    "# List of DataFrames to merge\n",
    "dfs_to_merge = [base_data_high_val, local_df, std_df,roam_df,isd_df,other_df,special_df,data_usage_df]\n",
    "\n",
    "# Merge the DataFrames using reduce() and merge()\n",
    "merged_df = reduce(lambda left, right: pd.merge(left, right, on='mobile_number'), dfs_to_merge)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop(columns='sep_vbc_3g',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d943a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f454dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13fb109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns with object data type\n",
    "object_columns = merged_df.select_dtypes(include='object')\n",
    "object_columns=object_columns.columns\n",
    "object_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c1ab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Dummy variables out of it \n",
    "dummy_vars = pd.get_dummies(merged_df[object_columns], drop_first=True, prefix=object_columns, prefix_sep='_')\n",
    "dummy_vars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f916d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_cols = dummy_vars.filter(regex='.*Others$').columns.to_list() # Using category 'Others' in each column as reference. \n",
    "dummy_vars.drop(columns=reference_cols, inplace=True)\n",
    "reference_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14b9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating dummy variables with original 'data'\n",
    "merged_df.drop(columns=object_columns, inplace=True) # dropping original categorical columns\n",
    "merged_df = pd.concat([merged_df, dummy_vars], axis=1)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0456feb",
   "metadata": {},
   "source": [
    "### This following section contains\n",
    "\n",
    "Test Train Split\n",
    "Class Imbalance\n",
    "Standardization\n",
    "- Modelling\n",
    "  - Model 1 : Logistic Regression with RFE & Manual Elimination ( Interpretable Model )\n",
    "  - Model 2 : PCA + Logistic Regression\n",
    "  - Model 3 : PCA + Random Forest Classifier\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2086d9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop(columns=['mobile_number'],inplace=True)\n",
    "# Replace inf with 0\n",
    "merged_df = merged_df.replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90810088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at quantiles from 0.90 to 1. \n",
    "merged_df.quantile(np.arange(0.9,1.01,0.01)).style.bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb875be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fbfd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9b36e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train-Test Split\n",
    "y = merged_df.pop('churn') # Predicted / Target Variable\n",
    "X = merged_df # Predictor variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fdddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e4b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio of classes \n",
    "class_0 = y[y == 0].count()\n",
    "class_1 = y[y == 1].count()\n",
    "\n",
    "print(f'Class Imbalance Ratio : {round(class_1/class_0,3)}')\n",
    "\n",
    "## We can use use SMOTE to make these imbalanced classes balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68f1029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install imblearn --trusted-host pypi.org --trusted-host files.pythonhosted.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d7d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smt = SMOTE(random_state=42, k_neighbors=5)\n",
    "\n",
    "# Resampling Train set to account for class imbalance\n",
    "\n",
    "X_train_resampled, y_train_resampled= smt.fit_resample(X_train, y_train)\n",
    "X_train_resampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2e7374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with numerical data\n",
    "condition1 = merged_df.dtypes == 'int'\n",
    "condition2 = merged_df.dtypes == 'float'\n",
    "numerical_vars = merged_df.columns[condition1 | condition2].to_list()\n",
    "# Standard scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "\n",
    "# Fit and transform train set \n",
    "X_train_resampled[numerical_vars] = scaler.fit_transform(X_train_resampled[numerical_vars])\n",
    "\n",
    "# Transform test set\n",
    "X_test[numerical_vars] = scaler.transform(X_test[numerical_vars])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2336ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics of standardized variables\n",
    "round(X_train_resampled.describe(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11ae5f1",
   "metadata": {},
   "source": [
    "### Baseline Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc4ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "baseline_model = LogisticRegression(random_state=100, class_weight='balanced') # `weight of class` balancing technique used\n",
    "baseline_model = baseline_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = baseline_model.predict_proba(X_train)[:,1]\n",
    "y_test_pred  = baseline_model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1eea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = pd.Series(y_train_pred,index = X_train.index, ) # converting test and train to a series to preserve index\n",
    "y_test_pred = pd.Series(y_test_pred,index = X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0309787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Baseline Performance Metrics\n",
    "import math\n",
    "def model_metrics(matrix) :\n",
    "    TN = matrix[0][0]\n",
    "    TP = matrix[1][1]\n",
    "    FP = matrix[0][1]\n",
    "    FN = matrix[1][0]\n",
    "    accuracy = round((TP + TN)/float(TP+TN+FP+FN),3)\n",
    "    print('Accuracy :' ,accuracy )\n",
    "    sensitivity = round(TP/float(FN + TP),3)\n",
    "    print('Sensitivity / True Positive Rate / Recall :', sensitivity)\n",
    "    specificity = round(TN/float(TN + FP),3)\n",
    "    print('Specificity / True Negative Rate : ', specificity)\n",
    "    precision = round(TP/float(TP + FP),3)\n",
    "    print('Precision / Positive Predictive Value :', precision)\n",
    "    print('F1-score :', round(2*precision*sensitivity/(precision + sensitivity),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f279f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction at threshold of 0.5 \n",
    "classification_threshold = 0.5 \n",
    "    \n",
    "y_train_pred_classified = y_train_pred.map(lambda x : 1 if x > classification_threshold else 0)\n",
    "y_test_pred_classified = y_test_pred.map(lambda x : 1 if x > classification_threshold else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc720f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "train_matrix = confusion_matrix(y_train, y_train_pred_classified)\n",
    "print('Confusion Matrix for train:\\n', train_matrix)\n",
    "test_matrix = confusion_matrix(y_test, y_test_pred_classified)\n",
    "print('\\nConfusion Matrix for test: \\n', test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8478de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model Performance : \n",
    "\n",
    "print('Train Performance : \\n')\n",
    "model_metrics(train_matrix)\n",
    "\n",
    "print('\\n\\nTest Performance : \\n')\n",
    "model_metrics(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c65b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specificity / Sensitivity Tradeoff \n",
    "\n",
    "# Classification at probability thresholds between 0 and 1 \n",
    "y_train_pred_thres = pd.DataFrame(index=X_train.index)\n",
    "thresholds = [float(x)/10 for x in range(10)]\n",
    "\n",
    "def thresholder(x, thresh) :\n",
    "    if x > thresh : \n",
    "        return 1 \n",
    "    else : \n",
    "        return 0\n",
    "\n",
    "    \n",
    "for i in thresholds:\n",
    "    y_train_pred_thres[i]= y_train_pred.map(lambda x : thresholder(x,i))\n",
    "y_train_pred_thres.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b200dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sensitivity, specificity, accuracy for each threshold\n",
    "metrics_df = pd.DataFrame(columns=['sensitivity', 'specificity', 'accuracy'])\n",
    "\n",
    "# Function for calculation of metrics for each threshold\n",
    "def model_metrics_thres(matrix) :\n",
    "    TN = matrix[0][0]\n",
    "    TP = matrix[1][1]\n",
    "    FP = matrix[0][1]\n",
    "    FN = matrix[1][0]\n",
    "    accuracy = round((TP + TN)/float(TP+TN+FP+FN),3)\n",
    "    sensitivity = round(TP/float(FN + TP),3)\n",
    "    specificity = round(TN/float(TN + FP),3)\n",
    "    return sensitivity,specificity,accuracy\n",
    "\n",
    "# generating a data frame for metrics for each threshold\n",
    "for thres,column in zip(thresholds,y_train_pred_thres.columns.to_list()) : \n",
    "    confusion = confusion_matrix(y_train, y_train_pred_thres.loc[:,column])\n",
    "    sensitivity,specificity,accuracy = model_metrics_thres(confusion)\n",
    "    \n",
    "    metrics_df =  metrics_df.append({ \n",
    "        'sensitivity' :sensitivity,\n",
    "        'specificity' : specificity,\n",
    "        'accuracy' : accuracy\n",
    "    }, ignore_index = True)\n",
    "    \n",
    "metrics_df.index = thresholds\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4bdd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.plot(kind='line', figsize=(24,8), grid=True, xticks=np.arange(0,1,0.02),\n",
    "                title='Specificity-Sensitivity TradeOff');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9afb303",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_cutoff = 0.45\n",
    "y_train_pred_final = y_train_pred.map(lambda x : 1 if x > optimum_cutoff else 0)\n",
    "y_test_pred_final = y_test_pred.map(lambda x : 1 if x > optimum_cutoff else 0)\n",
    "\n",
    "train_matrix = confusion_matrix(y_train, y_train_pred_final)\n",
    "print('Confusion Matrix for train:\\n', train_matrix)\n",
    "test_matrix = confusion_matrix(y_test, y_test_pred_final)\n",
    "print('\\nConfusion Matrix for test: \\n', test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eec06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Performance: \\n')\n",
    "model_metrics(train_matrix)\n",
    "\n",
    "print('\\n\\nTest Performance : \\n')\n",
    "model_metrics(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f1c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC_AUC score \n",
    "from sklearn.metrics import roc_auc_score\n",
    "print('ROC AUC score for Train : ',round(roc_auc_score(y_train, y_train_pred),3), '\\n' )\n",
    "print('ROC AUC score for Test : ',round(roc_auc_score(y_test, y_test_pred),3) )\n",
    "\n",
    "## Model is overfitting given we have a lot of variables present in it and its test-score is very less\n",
    "## It has very high TPR which is overfitting in case of the test data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f83f23",
   "metadata": {},
   "source": [
    "#### Generializing model building to improve on the model building and reduce overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5409f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "rfe = RFE(estimator=lr,n_features_to_select=25)\n",
    "results = rfe.fit(X_train,y_train)\n",
    "\n",
    "# DataFrame with features supported by RFE\n",
    "rfe_support = pd.DataFrame({'Column' : X.columns.to_list(), 'Rank' : rfe.ranking_, \n",
    "                                      'Support' :  rfe.support_}).sort_values(by=\n",
    "                                       'Rank', ascending=True)\n",
    "rfe_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8507ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE Selected columns\n",
    "rfe_selected_columns = rfe_support.loc[rfe_support['Rank'] == 1,'Column'].to_list()\n",
    "rfe_selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa66e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model with RFE columns\n",
    "import statsmodels.api as sm \n",
    "\n",
    "# Note that the SMOTE resampled Train set is used with statsmodels.api.GLM since it doesnot support class_weight\n",
    "logr = sm.GLM(y_train_resampled,(sm.add_constant(X_train_resampled[rfe_selected_columns])), family = sm.families.Binomial())\n",
    "logr_fit = logr.fit()\n",
    "logr_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee54cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using P-value and vif for manual feature elimination\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "def vif(X_train_resampled, logr_fit, selected_columns) : \n",
    "    vif = pd.DataFrame()\n",
    "    vif['Features'] = rfe_selected_columns\n",
    "    vif['VIF'] = [variance_inflation_factor(X_train_resampled[selected_columns].values, i) for i in range(X_train_resampled[selected_columns].shape[1])]\n",
    "    vif['VIF'] = round(vif['VIF'], 2)\n",
    "    vif = vif.set_index('Features')\n",
    "    vif['P-value'] = round(logr_fit.pvalues,4)\n",
    "    vif = vif.sort_values(by = [\"VIF\",'P-value'], ascending = [False,False])\n",
    "    return vif\n",
    "\n",
    "vif(X_train_resampled, logr_fit, rfe_selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f6e0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We have VIF of around 4, but there are few variables which have higher p-value , we can remove that variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing variable sachet_3g_7 based on p-values\n",
    "selected_columns = rfe_selected_columns\n",
    "selected_columns.remove('sachet_3g_7')\n",
    "selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf92087",
   "metadata": {},
   "outputs": [],
   "source": [
    "## building 2nd model \n",
    "logr2 = sm.GLM(y_train_resampled,(sm.add_constant(X_train_resampled[selected_columns])), family = sm.families.Binomial())\n",
    "logr2_fit = logr2.fit()\n",
    "logr2_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dbb025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vif and p-values\n",
    "vif(X_train_resampled, logr2_fit, selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acac99a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing std_ic_t2t_mou_7_perc based on p-value\n",
    "selected_columns.remove('std_ic_t2t_mou_7_perc')\n",
    "selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b99dc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building 3rd model \n",
    "logr3 = sm.GLM(y_train_resampled,(sm.add_constant(X_train_resampled[selected_columns])), family = sm.families.Binomial())\n",
    "logr3_fit = logr3.fit()\n",
    "logr3_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ef205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vif and p-values\n",
    "vif(X_train_resampled, logr3_fit, selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa8d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing sachet_delta_2g based on p-value\n",
    "selected_columns.remove('sachet_delta_2g')\n",
    "selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e052040",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building 4th model \n",
    "logr4 = sm.GLM(y_train_resampled,(sm.add_constant(X_train_resampled[selected_columns])), family = sm.families.Binomial())\n",
    "logr4_fit = logr4.fit()\n",
    "logr4_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e298a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vif and p-values\n",
    "vif(X_train_resampled, logr4_fit, selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e497281",
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing loc_ic_t2m_mou_8_perc based on VIF value \n",
    "selected_columns.remove('loc_ic_t2m_mou_8_perc')\n",
    "## Building 5th model \n",
    "logr5 = sm.GLM(y_train_resampled,(sm.add_constant(X_train_resampled[selected_columns])), family = sm.families.Binomial())\n",
    "logr5_fit = logr5.fit()\n",
    "# vif and p-values\n",
    "vif(X_train_resampled, logr5_fit, selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59313c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## All the model values are lesser than 4 in VIF and p-value in range of 0.05 so this will be our final model \n",
    "## to work with \n",
    "logr5_fit.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c213b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction \n",
    "y_train_pred_lr = logr5_fit.predict(sm.add_constant(X_train_resampled[selected_columns]))\n",
    "y_train_pred_lr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74f8990",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_lr = logr5_fit.predict(sm.add_constant(X_test[selected_columns]))\n",
    "y_test_pred_lr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610c3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance\n",
    "## finding the sensitivity and specificity with respect to the thresholds \n",
    "# Specificity / Sensitivity Tradeoff \n",
    "\n",
    "# Classification at probability thresholds between 0 and 1 \n",
    "y_train_pred_thres = pd.DataFrame(index=X_train_resampled.index)\n",
    "thresholds = [float(x)/10 for x in range(10)]\n",
    "\n",
    "def thresholder(x, thresh) :\n",
    "    if x > thresh : \n",
    "        return 1 \n",
    "    else : \n",
    "        return 0\n",
    "\n",
    "    \n",
    "for i in thresholds:\n",
    "    y_train_pred_thres[i]= y_train_pred_lr.map(lambda x : thresholder(x,i))\n",
    "y_train_pred_thres.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a183a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame for Performance metrics at each threshold\n",
    "\n",
    "logr_metrics_df = pd.DataFrame(columns=['sensitivity', 'specificity', 'accuracy'])\n",
    "for thres,column in zip(thresholds,y_train_pred_thres.columns.to_list()) : \n",
    "    confusion = confusion_matrix(y_train_resampled, y_train_pred_thres.loc[:,column])\n",
    "    sensitivity,specificity,accuracy = model_metrics_thres(confusion)\n",
    "    logr_metrics_df =  logr_metrics_df.append({ \n",
    "        'sensitivity' :sensitivity,\n",
    "        'specificity' : specificity,\n",
    "        'accuracy' : accuracy\n",
    "    }, ignore_index = True)\n",
    "    \n",
    "logr_metrics_df.index = thresholds\n",
    "logr_metrics_df\n",
    "\n",
    "## at 0.5 we have the best sensitivity and specificity and accuracy based on how close these values are \n",
    "## range from 0.4 to 0.5 is acting really greate place to start with model performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6486682",
   "metadata": {},
   "outputs": [],
   "source": [
    "logr_metrics_df.plot(kind='line', figsize=(24,8), grid=True, xticks=np.arange(0,1,0.02),\n",
    "                title='Specificity-Sensitivity TradeOff');\n",
    "\n",
    "## This  graph is very specifically crossing at around 0.5 as threshold, which should be the go to place \n",
    "#3 for the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38649e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_cutoff = 0.497\n",
    "y_train_pred_lr_final = y_train_pred_lr.map(lambda x : 1 if x > optimum_cutoff else 0)\n",
    "y_test_pred_lr_final = y_test_pred_lr.map(lambda x : 1 if x > optimum_cutoff else 0)\n",
    "\n",
    "train_matrix = confusion_matrix(y_train_resampled, y_train_pred_lr_final)\n",
    "print('Confusion Matrix for train:\\n', train_matrix)\n",
    "test_matrix = confusion_matrix(y_test, y_test_pred_lr_final)\n",
    "print('\\nConfusion Matrix for test: \\n', test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eaca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Performance: \\n')\n",
    "model_metrics(train_matrix)\n",
    "\n",
    "print('\\n\\nTest Performance : \\n')\n",
    "model_metrics(test_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b357629",
   "metadata": {},
   "source": [
    "Now we have reduced the overfitting after removing the columns and also we can se how our model is predicting the classes \n",
    "correctly but it has very low precision, which might be that case tha our over-samling technique needs some tuning to make\n",
    "it better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439746ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC_AUC score \n",
    "print('ROC AUC score for Train : ',round(roc_auc_score(y_train_resampled, y_train_pred_lr),3), '\\n' )\n",
    "print('ROC AUC score for Test : ',round(roc_auc_score(y_test, y_test_pred_lr),3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd977f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interpretable model summary\n",
    "lr_summary_html = logr5_fit.summary().tables[1].as_html()\n",
    "lr_results = pd.read_html(lr_summary_html, header=0, index_col=0)[0]\n",
    "coef_column = lr_results.columns[0]\n",
    "print('Most important predictors of Churn , in order of importance and their coefficients are as follows : \\n')\n",
    "lr_results.sort_values(by=coef_column, key=lambda x: abs(x), ascending=False)['coef']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11258ad5",
   "metadata": {},
   "source": [
    "### Building Non-linear model using PCA and boosting algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041691d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "pca = PCA(random_state = 42) \n",
    "pca.fit(X_train_resampled) # note that pca is fit on resampled train set. \n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc2fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bccfa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scree Plot to understand variance by components\n",
    "var_cum = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.set_style('darkgrid')\n",
    "sns.lineplot(np.arange(1,len(var_cum) + 1), var_cum)\n",
    "plt.xticks(np.arange(0,140,5))\n",
    "plt.axhline(0.8,color='g')\n",
    "plt.axhline(0.95,color='r')\n",
    "plt.axhline(1.0,color='r')\n",
    "plt.axvline(25,color='b')\n",
    "plt.axvline(50,color='b')\n",
    "plt.axvline(100,color='b')\n",
    "plt.text(10,0.81,'0.80')\n",
    "plt.text(10,0.96,'0.95')\n",
    "\n",
    "plt.title('Scree Plot of Telecom Churn Train Set');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483ffbcd",
   "metadata": {},
   "source": [
    "Variables above 100 and less than 105 explains about 95% of the total variance in the data set, we can now go ahead with use of 105 components to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18bc8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA using the first 55 components\n",
    "pca_final = PCA(n_components=105, random_state=42)\n",
    "transformed_data = pca_final.fit_transform(X_train_resampled)\n",
    "X_train_pca = pd.DataFrame(transformed_data, columns=[\"PC_\"+str(x) for x in range(1,106)], index = X_train_resampled.index)\n",
    "data_train_pca = pd.concat([X_train_pca, y_train_resampled], axis=1)\n",
    "\n",
    "data_train_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b978db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting principal components \n",
    "sns.pairplot(data=data_train_pca, x_vars=[\"PC_1\"], y_vars=[\"PC_2\"], hue = \"churn\", size=8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bd4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,y Split\n",
    "y_train_pca = data_train_pca.pop('churn')\n",
    "X_train_pca = data_train_pca\n",
    "\n",
    "# Transforming test set with pca ( 45 components)\n",
    "X_test_pca = pca_final.transform(X_test)\n",
    "\n",
    "# Logistic Regression\n",
    "lr_pca = LogisticRegression(random_state=100, class_weight='balanced')\n",
    "lr_pca.fit(X_train_pca,y_train_pca ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ea8297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train predictions\n",
    "y_train_pred_lr_pca = lr_pca.predict(X_train_pca)\n",
    "y_train_pred_lr_pca[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eebc929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Prediction\n",
    "X_test_pca = pca_final.transform(X_test)\n",
    "y_test_pred_lr_pca = lr_pca.predict(X_test_pca)\n",
    "y_test_pred_lr_pca[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e4212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = confusion_matrix(y_train_resampled, y_train_pred_lr_pca)\n",
    "test_matrix = confusion_matrix(y_test, y_test_pred_lr_pca)\n",
    "\n",
    "print('Train Performance :\\n')\n",
    "model_metrics(train_matrix)\n",
    "\n",
    "print('\\nTest Performance :\\n')\n",
    "model_metrics(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea1a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Logistic regression model using pca transformed train set\n",
    "from sklearn.pipeline import Pipeline\n",
    "lr_pca = LogisticRegression(random_state=100, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddc6dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV , StratifiedKFold\n",
    "params = {\n",
    "    'penalty' : ['l1','l2','none'], \n",
    "    'C' : [0,1,2,3,4,5,10,50]\n",
    "}\n",
    "folds = StratifiedKFold(n_splits=4, shuffle=True, random_state=100)\n",
    "\n",
    "search = GridSearchCV(cv=folds, estimator = lr_pca, param_grid=params,scoring='roc_auc', verbose=True)\n",
    "search.fit(X_train_pca, y_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa383de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimum Hyperparameters\n",
    "print('Best ROC-AUC score :', search.best_score_)\n",
    "print('Best Parameters :', search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c1bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling using the best LR-PCA estimator \n",
    "lr_pca_best = search.best_estimator_\n",
    "lr_pca_best_fit = lr_pca_best.fit(X_train_pca, y_train_pca)\n",
    "\n",
    "# Prediction on Train set\n",
    "y_train_pred_lr_pca_best = lr_pca_best_fit.predict(X_train_pca)\n",
    "y_train_pred_lr_pca_best[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d1336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test set\n",
    "y_test_pred_lr_pca_best = lr_pca_best_fit.predict(X_test_pca)\n",
    "y_test_pred_lr_pca_best[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eb5c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Performance after Hyper Parameter Tuning\n",
    "\n",
    "train_matrix = confusion_matrix(y_train_resampled, y_train_pred_lr_pca_best)\n",
    "test_matrix = confusion_matrix(y_test, y_test_pred_lr_pca_best)\n",
    "print(test_matrix)\n",
    "\n",
    "print('Train Performance :\\n')\n",
    "model_metrics(train_matrix)\n",
    "\n",
    "print('\\nTest Performance :\\n')\n",
    "model_metrics(test_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651453ed",
   "metadata": {},
   "source": [
    "As it was expected, linear model with PCA will fail to work better , given we are overfitting the data and also due to orthogonal transformation of the variables , linear function fails to work efficiently, we can try with some tree models and we can use boosting or bagging to improve the fitting of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a091cea",
   "metadata": {},
   "source": [
    "### PCA with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c9596",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0734373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# creating a random forest classifier using pca output\n",
    "\n",
    "pca_rf = RandomForestClassifier(random_state=42, class_weight= {0 :0.5 , 1 : 0.5 } , oob_score=True, n_jobs=-1,verbose=1)\n",
    "pca_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d3f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter Tuning\n",
    "params = {\n",
    "    'n_estimators'  : [50,100],\n",
    "    'max_depth' : [5,6,7],\n",
    "    'min_samples_leaf' : [25,30]\n",
    "}\n",
    "folds = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "pca_rf_model_search = GridSearchCV(estimator=pca_rf, param_grid=params, \n",
    "                                   cv=folds, scoring='f1', verbose=False)\n",
    "\n",
    "pca_rf_model_search.fit(X_train_pca, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c39ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimum Hyperparameters\n",
    "print('Best F1 score :', pca_rf_model_search.best_score_)\n",
    "print('Best Parameters :', pca_rf_model_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23d7ec8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_rf_model_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04388993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling using the best PCA-RandomForest Estimator \n",
    "pca_rf_best = pca_rf_model_search.best_estimator_\n",
    "pca_rf_best_fit = pca_rf_best.fit(X_train_pca, y_train_resampled)\n",
    "\n",
    "# Prediction on Train set\n",
    "y_train_pred_pca_rf_best = pca_rf_best_fit.predict(X_train_pca)\n",
    "y_train_pred_pca_rf_best[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6ad1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test set\n",
    "y_test_pred_pca_rf_best = pca_rf_best_fit.predict(X_test_pca)\n",
    "y_test_pred_pca_rf_best[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d437de45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA - RandomForest Model Performance - Hyper Parameter Tuned\n",
    "\n",
    "train_matrix = confusion_matrix(y_train_resampled, y_train_pred_pca_rf_best)\n",
    "test_matrix = confusion_matrix(y_test, y_test_pred_pca_rf_best)\n",
    "\n",
    "print('Train Performance :\\n')\n",
    "model_metrics(train_matrix)\n",
    "\n",
    "print('\\nTest Performance :\\n')\n",
    "model_metrics(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a1071",
   "metadata": {},
   "outputs": [],
   "source": [
    "## out of bag error \n",
    "pca_rf_best_fit.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aa0913",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recommendations\n",
    "print('Most Important Predictors of churn , in the order of importance are : ')\n",
    "lr_results.sort_values(by=coef_column, key=lambda x: abs(x), ascending=False)['coef']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a516586",
   "metadata": {},
   "source": [
    "From the above, the following are the strongest indicators of churn are:\n",
    "\n",
    "\n",
    "- Roaming customers have higher chance to churn, providing roaming prices might not be as efficient. Ones opting out of roaming has better chance to not churn\n",
    "- Increase in incoming minutes of usage in the action phase can reduce churn rate.\n",
    "- Customers who churn show lower average monthly local incoming calls from fixed line in the action period, local outgoing calls to mobile operators and local incoming from same operator gives lesser churn.\n",
    "- Customers who churn show lower number of recharges done in action period by .94 standard deviations, when all other factors are held constant. This is the second strongest indicator of churn.\n",
    "- Further customers who churn have done 0.6 standard deviations higher recharge than non-churn customers in good phase. This factor when coupled with above factors is a good indicator of churn.\n",
    "- STD & ISD calling has resulted in increase of churn, these facilities might have price points which are not attractive for the users\n",
    "- Other services result in the higher churning, these after/other services need an important input to be checked and maintained\n",
    "\n",
    "\n",
    "#### Model Selection\n",
    "- Models with high sensitivity are the best for predicting churn. Use the PCA + Logistic Regression model to predict churn. It has an ROC score of 0.93, test sensitivity of 75.9%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
